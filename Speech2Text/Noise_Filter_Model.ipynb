{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### EĞİTİLMESİ GEREK HEPSİNİ ÇALITIRINCA EĞİTİLİYOR",
   "id": "e98b272dbad19603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "train_path = \"Datasets/Filter_Datasets/train.csv\"\n",
    "test_path = \"Datasets/Filter_Datasets/test.csv\""
   ],
   "id": "2ae3cfa2e1511e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_and_sample(csv_path, fraction):\n",
    "    data = pd .read_csv(csv_path)\n",
    "    sample_data = data.sample(frac=fraction, random_state=42)\n",
    "    return sample_data"
   ],
   "id": "7996df7a24cd3e78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fraction = 1\n",
    "\n",
    "train_data = read_and_sample(train_path,fraction)\n",
    "test_data = read_and_sample(test_path,fraction)"
   ],
   "id": "2de61f56c3377a58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data.head()",
   "id": "32ddf89813f80e4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target_sr = 48000\n",
    "\n",
    "def data_clean(audio,sr):\n",
    "    audio = librosa.resample(audio,orig_sr=sr,target_sr=target_sr)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    audio_feature = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "    audio_feature = librosa.power_to_db(S=audio_feature)\n",
    "    return audio_feature"
   ],
   "id": "c69b4854bb85f2dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def data_specktrogram(data):\n",
    "    x_list, y_list = [], []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        audio, sample_rate = librosa.load(row[\"combined_path\"])\n",
    "        tensor_x = torch.from_numpy(data_clean(audio,sample_rate)).T.to(device)\n",
    "        audio, sample_rate = librosa.load(row[\"clean_path\"])\n",
    "        tensor_y = torch.from_numpy(data_clean(audio,sample_rate)).T.to(device)\n",
    "        x_list.append(tensor_x.unsqueeze(0))\n",
    "        y_list.append(tensor_y.unsqueeze(0))\n",
    "    return x_list, y_list"
   ],
   "id": "62f7190552b368a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_x_list , train_y_list = data_specktrogram(train_data)",
   "id": "3cbabf3d5c610467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_x_list , test_y_list = data_specktrogram(test_data)",
   "id": "4c64c02919f6b97c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, layer_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.layer_size = layer_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, layer_size, batch_first=True).to(device)\n",
    "        self.fc = nn.Linear(hidden_size, output_size).to(device)\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_size, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.layer_size, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        output, (hx, cx) = self.lstm(x, (h0, c0))\n",
    "        output = self.fc(output)\n",
    "        return output"
   ],
   "id": "845c4689a98ae1ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_size = train_x_list[0].size(-1)\n",
    "output_size = train_y_list[0].size(-1)\n",
    "hidden_size = 256\n",
    "layer_size = 5\n",
    "\n",
    "model = LSTM(input_size, hidden_size, layer_size, output_size).to(device)"
   ],
   "id": "4d6a03690c7f2650",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "epochs = 10000\n",
    "lr = 0.001\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "id": "e162c7a272fa90d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "iteration_history = list()\n",
    "loss_history = list()\n",
    "count = 0\n",
    "show_loss = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for audio, labels in zip(train_x_list, train_y_list):\n",
    "        audio, labels = audio.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(audio)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        count += 1\n",
    "        if count % show_loss == 0:\n",
    "            iteration_history.append(count)\n",
    "            loss_history.append(loss.item())\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch, epochs, loss.item()))\n",
    "            torch.save(model,\"Checkpoints\")"
   ],
   "id": "5c6b011fa4b2ead0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iteration_history, loss_history, label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5aabbc81fea0840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "for audio, labels in zip(test_x_list, test_y_list):\n",
    "    audio, labels = audio.to(device), labels.to(device)\n",
    "    outputs = model(audio)\n",
    "    \n",
    "    is_close = torch.isclose(outputs, labels)\n",
    "    correct += is_close.sum().item()\n",
    "    total += labels.numel()\n",
    "accuracy = 100 * correct / total\n",
    "print(accuracy)"
   ],
   "id": "fe91c28f26fab7aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac87e518f9e7cc2e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
