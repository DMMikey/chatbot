{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T07:59:28.686913Z",
     "start_time": "2024-05-13T07:59:23.775789Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "46bfa2bd95146855",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "datasets_tr = load_dataset(\"covost2\", \"tr_en\", data_dir=\"Datasets/STT_Datasets/tr\")",
   "id": "cd5d535cc3190ca5",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data = datasets_tr[\"train\"][:1]",
   "id": "67ede4d64b2f2856",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "data",
   "id": "b7d083ee7386fc74",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def audio_transformer(audio_path):\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "    audio = librosa.util.normalize(audio)\n",
    "    audio = librosa.feature.melspectrogram(audio)\n",
    "    audio = librosa.power_to_db(audio, ref=np.max)\n",
    "    tensor = torch.from_numpy(audio).T\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    return tensor"
   ],
   "id": "4228a886d90f9f3b",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6a28dce04f109ed2",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_x_list,train_y_list=[],[]\n",
    "for path,label in zip(data[\"file\"],data[\"translation\"]):\n",
    "    train_x_list.append(audio_transformer(path))\n",
    "    train_y_list.append(label_processing(label))"
   ],
   "id": "deb80f947fe57737",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "id": "4c14202938f26836",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,layer_size, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size,layer_size, batch_first=True, dropout=p)\n",
    "    def forward(self, x):\n",
    "        output = self.drop(x)\n",
    "        output, hidden = self.rnn(output)\n",
    "        return output, hidden"
   ],
   "id": "82cb0d30e70131c1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.EO = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Hi = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, keys,query):\n",
    "        hidden_state = query[0].permute(1,0,2)\n",
    "        cell_state = query[1].permute(1,0,2)\n",
    "        \n",
    "        hidden_state = torch.cat((hidden_state, cell_state), dim=-1)\n",
    "        scores = self.Va(torch.tanh(self.Hi(hidden_state) + self.EO(keys)))\n",
    "        \n",
    "        weights = F.softmax(scores, dim=1)\n",
    "        context = torch.sum(torch.mul(weights,keys), dim=1).unsqueeze(1)\n",
    "        \n",
    "        return context"
   ],
   "id": "c832b7641be113de",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,hidden_size,output_size,layer_size,p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(output_size,hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size*2, hidden_size,layer_size, batch_first=True, dropout=p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attention = Attention(hidden_size)\n",
    "        \n",
    "    def forward(self,decoder_input, decoder_hidden, encoder_output):\n",
    "        output = self.drop(self.embedding(decoder_input))\n",
    "        \n",
    "        context = self.attention(encoder_output, decoder_hidden)\n",
    "        input_rnn = torch.cat((output, context), dim=-1)\n",
    "        \n",
    "        output, hidden = self.rnn(input_rnn,decoder_hidden)\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, hidden"
   ],
   "id": "4e320b33c13d38c",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,layer_size, output_size, p, max_length):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.drop = nn.Dropout(p)\n",
    "        self.encoder = Encoder(input_size, hidden_size,layer_size,p)\n",
    "        self.decoder = Decoder(hidden_size,output_size,layer_size,p)\n",
    "        \n",
    "    def forward(self,x,target=None):\n",
    "        encoder_output,encoder_hidden = self.encoder(x)\n",
    "        \n",
    "        decoder_outputs = []\n",
    "        \n",
    "        decoder_input = torch.empty(x.size(0),1,dtype=torch.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        if target is not None:\n",
    "            max_length = target.size(1)\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target is not None:\n",
    "                decoder_input = target[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                topv,topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(1).detach()\n",
    "            \n",
    "                if decoder_input == EOS_token:\n",
    "                    break\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs,1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs,dim=1)\n",
    "        return decoder_outputs"
   ],
   "id": "3713d5d0a7d4041e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = Model(128,256,1,len(alphabet)+2,0.2,100)",
   "id": "cf02da3ed9ef6755",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=4e-5,weight_decay=1e-5)\n",
    "epochs = 1000"
   ],
   "id": "b38ed6cd7fa74180",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for e in range(epochs):\n",
    "    for i,(x,y) in enumerate(zip(train_x_list,train_y_list)):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x,y).squeeze(0)\n",
    "        loss = criterion(output,y.squeeze(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = \"\"\n",
    "        for i in range(len(output)):\n",
    "            value = torch.argmax(output[i])\n",
    "            prediction += alphabet[value-2]\n",
    "        print(loss.item(),prediction)"
   ],
   "id": "bfcbbe7cf5d4c4f6",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "len(data[\"translation\"][0]) , data[\"translation\"][0]",
   "id": "140a01fd841dc58e",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_y_list[0].size(1)",
   "id": "7e1f3874ab17ff81",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pred = model(train_x_list[0])\n",
    "pred.shape"
   ],
   "id": "97c32e5b9fd807b3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "prediction",
   "id": "94c29c0b844a4e51",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e6da31fe531b4d5b",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
