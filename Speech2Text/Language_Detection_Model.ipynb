{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### YAPILMIŞTI DÜZENLENECEK (VAD ve FILTERDAN SONRA)",
   "id": "f2b50b7bdaf7efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "# Gerekli kütüphaneler\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import warnings        \n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "a641a63b3d77cb6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:40.795268Z",
     "start_time": "2024-04-25T17:31:40.785692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Veri işleme fonksiyonu\n",
    "def read_and_sample(csv_path, fraction):\n",
    "    data = pd.read_csv(csv_path)\n",
    "    sample_data = data.sample(frac=fraction, random_state=42)\n",
    "    return sample_data"
   ],
   "id": "d4b9263625371b67",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:41.186452Z",
     "start_time": "2024-04-25T17:31:40.797261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verileri oku\n",
    "sample_fraction = 1\n",
    "\n",
    "train_data = read_and_sample(\"Datasets/train.csv\", sample_fraction)\n",
    "val_data = read_and_sample(\"Datasets/val.csv\", sample_fraction)\n",
    "test_data = read_and_sample(\"Datasets/test.csv\", sample_fraction)"
   ],
   "id": "e7d7b5af6628f184",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:42.224832Z",
     "start_time": "2024-04-25T17:31:42.212930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Verileri Kontrol et\n",
    "print(\"Train Data Sample:\")\n",
    "print(train_data.head())\n",
    "print(\"Validation Data Sample:\")\n",
    "print(val_data.head())\n",
    "print(\"Test Data Sample:\")\n",
    "print(test_data.head())"
   ],
   "id": "2fe7bd1886b93024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Sample:\n",
      "                                                features  labels\n",
      "11456  Datasets\\zh-CN\\clips\\common_voice_zh-CN_189554...       1\n",
      "1288   Datasets\\zh-CN\\clips\\common_voice_zh-CN_188230...       1\n",
      "5535   Datasets\\zh-CN\\clips\\common_voice_zh-CN_189779...       1\n",
      "12657  Datasets\\zh-CN\\clips\\common_voice_zh-CN_190086...       1\n",
      "8603      Datasets\\tr\\clips\\common_voice_tr_17527179.mp3       0\n",
      "Validation Data Sample:\n",
      "                                               features  labels\n",
      "1652  Datasets\\zh-CN\\clips\\common_voice_zh-CN_189997...       1\n",
      "5303  Datasets\\zh-CN\\clips\\common_voice_zh-CN_189834...       1\n",
      "2987  Datasets\\zh-CN\\clips\\common_voice_zh-CN_198784...       1\n",
      "1545     Datasets\\tr\\clips\\common_voice_tr_17529087.mp3       0\n",
      "3767  Datasets\\zh-CN\\clips\\common_voice_zh-CN_186595...       1\n",
      "Test Data Sample:\n",
      "                                               features  labels\n",
      "1057     Datasets\\tr\\clips\\common_voice_tr_17483907.mp3       0\n",
      "812      Datasets\\tr\\clips\\common_voice_tr_19190338.mp3       0\n",
      "2658     Datasets\\tr\\clips\\common_voice_tr_18377185.mp3       0\n",
      "809   Datasets\\zh-CN\\clips\\common_voice_zh-CN_187717...       1\n",
      "862   Datasets\\zh-CN\\clips\\common_voice_zh-CN_189955...       1\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:31:54.293204Z",
     "start_time": "2024-04-25T17:31:54.286225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Etiket dönüşüm fonksiyonu\n",
    "def labels(label):\n",
    "    return tf.constant([1, 0] if label == 0 else [0, 1], dtype=tf.int32)"
   ],
   "id": "6b41c387f8faef0d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:34.855785Z",
     "start_time": "2024-04-25T17:32:07.098839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train verileri yükle\n",
    "train_x_list, train_y_list = [], []\n",
    "\n",
    "for index, row in train_data.iterrows():\n",
    "    audio, sample_rate = librosa.load(row['features'])\n",
    "    audio_feature = librosa.feature.melspectrogram(y=audio*2, sr=sample_rate)\n",
    "    tensor_x = tf.transpose(tf.convert_to_tensor(audio_feature, dtype=tf.float32))\n",
    "    tensor_y = labels(row['labels'])\n",
    "    train_x_list.append(tensor_x)\n",
    "    train_y_list.append(tensor_y)"
   ],
   "id": "44ebf429f2b2a482",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:42.462630Z",
     "start_time": "2024-04-25T17:32:34.857790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# val verileri yükle\n",
    "val_x_list, val_y_list = [], []\n",
    "\n",
    "for index, row in val_data.iterrows():\n",
    "    audio, sample_rate = librosa.load(row['features'])\n",
    "    audio_feature = librosa.feature.melspectrogram(y=audio*2, sr=sample_rate)\n",
    "    tensor_x = tf.transpose(tf.convert_to_tensor(audio_feature, dtype=tf.float32))\n",
    "    tensor_y = labels(row['labels'])\n",
    "    val_x_list.append(tensor_x)\n",
    "    val_y_list.append(tensor_y)"
   ],
   "id": "64f69a5d05ea521e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:46.845756Z",
     "start_time": "2024-04-25T17:32:46.839613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parametreleri ayarla\n",
    "batch_size = 64\n",
    "epochs = 1000\n",
    "input_size = 128\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "learning_rate=1e-4"
   ],
   "id": "138249653ecd457b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:32:58.501053Z",
     "start_time": "2024-04-25T17:32:58.444360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train_dataset oluştur\n",
    "def generator_train():\n",
    "    for features, labels in zip(train_x_list, train_y_list):\n",
    "        yield (features, labels)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_generator(\n",
    "    generator_train,\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=((None, input_size), (output_size))\n",
    ")\n",
    "\n",
    "train_dataset = train_dataset.padded_batch(batch_size=batch_size, padded_shapes=([None, input_size],[output_size]))"
   ],
   "id": "215a6c4daa098aab",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:33:06.666231Z",
     "start_time": "2024-04-25T17:33:06.585141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# val_dataset oluştur\n",
    "def generator_val():\n",
    "    for features, labels in zip(val_x_list, val_y_list):\n",
    "        yield (features, labels)\n",
    "        \n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    generator_val,\n",
    "    output_types=(tf.float32, tf.int32),\n",
    "    output_shapes=((None, input_size), (output_size))\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.padded_batch(batch_size=batch_size, padded_shapes=([None, input_size], [output_size]))"
   ],
   "id": "cddc38398b04fc37",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:33:16.996549Z",
     "start_time": "2024-04-25T17:33:16.708725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modeli oluştur\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(None, input_size), batch_size=batch_size),\n",
    "    tf.keras.layers.LSTM(hidden_size,kernel_regularizer=tf.keras.regularizers.L2()),\n",
    "    tf.keras.layers.Dense(output_size, activation='linear',kernel_regularizer=tf.keras.regularizers.L2())\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              metrics=['accuracy'])"
   ],
   "id": "b2c87a96fe1269ea",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:33:25.444109Z",
     "start_time": "2024-04-25T17:33:25.393520Z"
    }
   },
   "cell_type": "code",
   "source": "model.summary()",
   "id": "88b3047e3b9f6201",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001B[38;5;33mLSTM\u001B[0m)                     │ (\u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m128\u001B[0m)              │       \u001B[38;5;34m131,584\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ (\u001B[38;5;34m64\u001B[0m, \u001B[38;5;34m2\u001B[0m)                │           \u001B[38;5;34m258\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m131,842\u001B[0m (515.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,842</span> (515.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m131,842\u001B[0m (515.01 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,842</span> (515.01 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T17:35:08.018590Z",
     "start_time": "2024-04-25T17:34:32.152822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Modeli eğit\n",
    "checkpoint_path = \"Checkpoints/cp.weights.h5\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "history = model.fit(train_dataset,  \n",
    "          epochs=epochs,\n",
    "          validation_data=val_dataset,\n",
    "          callbacks=[cp_callback])"
   ],
   "id": "1bca49c08a24e342",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      4/Unknown \u001B[1m6s\u001B[0m 575ms/step - accuracy: 0.5496 - loss: 2.7821\n",
      "Epoch 1: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 1s/step - accuracy: 0.5543 - loss: 2.7812 - val_accuracy: 0.5909 - val_loss: 2.7605\n",
      "Epoch 2/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 559ms/step - accuracy: 0.6155 - loss: 2.7559\n",
      "Epoch 2: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 734ms/step - accuracy: 0.6139 - loss: 2.7551 - val_accuracy: 0.5909 - val_loss: 2.7352\n",
      "Epoch 3/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 615ms/step - accuracy: 0.6155 - loss: 2.7303\n",
      "Epoch 3: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 770ms/step - accuracy: 0.6139 - loss: 2.7295 - val_accuracy: 0.5909 - val_loss: 2.7095\n",
      "Epoch 4/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 548ms/step - accuracy: 0.6155 - loss: 2.7044\n",
      "Epoch 4: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 719ms/step - accuracy: 0.6139 - loss: 2.7036 - val_accuracy: 0.5909 - val_loss: 2.6834\n",
      "Epoch 5/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 536ms/step - accuracy: 0.6155 - loss: 2.6780\n",
      "Epoch 5: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 705ms/step - accuracy: 0.6139 - loss: 2.6772 - val_accuracy: 0.5909 - val_loss: 2.6570\n",
      "Epoch 6/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 728ms/step - accuracy: 0.6155 - loss: 2.6513\n",
      "Epoch 6: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 1s/step - accuracy: 0.6139 - loss: 2.6505 - val_accuracy: 0.5909 - val_loss: 2.6303\n",
      "Epoch 7/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 568ms/step - accuracy: 0.6155 - loss: 2.6244\n",
      "Epoch 7: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 744ms/step - accuracy: 0.6139 - loss: 2.6236 - val_accuracy: 0.5909 - val_loss: 2.6035\n",
      "Epoch 8/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 630ms/step - accuracy: 0.6155 - loss: 2.5972\n",
      "Epoch 8: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 805ms/step - accuracy: 0.6139 - loss: 2.5964 - val_accuracy: 0.5909 - val_loss: 2.5765\n",
      "Epoch 9/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 571ms/step - accuracy: 0.6155 - loss: 2.5699\n",
      "Epoch 9: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 743ms/step - accuracy: 0.6139 - loss: 2.5691 - val_accuracy: 0.5909 - val_loss: 2.5493\n",
      "Epoch 10/10\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 566ms/step - accuracy: 0.6155 - loss: 2.5423\n",
      "Epoch 10: saving model to Checkpoints/cp.weights.h5\n",
      "\u001B[1m4/4\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 751ms/step - accuracy: 0.6139 - loss: 2.5415 - val_accuracy: 0.5909 - val_loss: 2.5219\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loss grafiği çizdir\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(loss, label='Loss')\n",
    "ax.plot(val_loss, label='Validation Loss')\n",
    "ax.set_title('Loss')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "97ba098f12862ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Validation Accuracy grafiği çizdir\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(val_acc, label='Validation Accuracy')\n",
    "ax.set_title('Validation Accuracy')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ],
   "id": "bd8b5860c20de1d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test verileri yükle\n",
    "test_x_list, test_y_list = [], []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    audio, sample_rate = librosa.load(row['features'])\n",
    "    audio_feature = librosa.feature.melspectrogram(y=audio*2, sr=sample_rate)\n",
    "    tensor_x = tf.transpose(tf.convert_to_tensor(audio_feature, dtype=tf.float32))\n",
    "    tensor_y = labels(row['labels'])\n",
    "    test_x_list.append(tensor_x)\n",
    "    test_y_list.append(tensor_y)\n",
    "    print(tensor_x.shape, tensor_y.shape)"
   ],
   "id": "3ee84b67c934113d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test_dataset oluştur\n",
    "def generator_test():\n",
    "    for features, labels in zip(test_x_list, test_y_list):\n",
    "        yield (features, labels)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    generator_test,\n",
    "    output_types=(tf.float32, tf.int16),\n",
    "    output_shapes=((None, input_size), (output_size))\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.padded_batch(batch_size=batch_size, padded_shapes=([None, input_size], [output_size]))"
   ],
   "id": "547280145bcdc421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loss, acc = model.evaluate(test_dataset, verbose=2)\n",
    "print(\"Untrained model, accuracy: {:5.2f}%\".format(100 * acc))"
   ],
   "id": "731b88e2c3074701",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
